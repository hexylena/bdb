---
author: Helena Rasche
date: 2022-03-21
title: "Debugging, Tracing, and Fading: Improving the Mental Model"
titlepage: true
listings: true
logo: ../avans.jpg
bibliography: poster.bib
colorlinks: true
abstract: |
    Students have a poor working model of code which leads to numerous issues;
    they struggle to debug, to adjust copy-pasted code, and to predict runtime
    behaviour.

    Existing research provides a number of avenues for improving our coding
    education and improving the student experience. Here we discuss a number of
    strategies to enhance student learning, provide a deeper understanding of
    code, and improve the student mental model of code execution. This should
    allow students to thrive as budding coders.
---

# The Problem

Currently students finish the course with a moderate working understanding of
code, not sufficient to write their own programs or adapt new code to their
situation. This is an extremely suboptimal outcome, after 8 weeks they
should be able to write moderately complex python programs.

Showing what happens live on the screen is received well by students, if
they can manage to watch what we type and try to type it themselves
simultaneously. We know at least that our examples give the correct result,
but students never see anything other than correct, working code, and never
have to formulate an internal model for how to write code. They end up
copying and pasting and not understanding *why*. As of now nothing is going
"well", and there is room for improvement in all aspects, but specifically
this discussion will focus on improving mental models of code and the
potential effects on understanding code execution.

Predicting code behaviour without running it is a key component of work as a
programmer, and a lot of the time we spend debugging relies on us emulating
the computer in our head. Without a solid mental model of code behaviour
one cannot predict how it will function in one situation, much less other or
non-standard situations. Planning for code to handle both good and bad
inputs requires some creativity and mentally planning around expected values
at various points throughout the execution.

This situation leaves students unprepared for incorrect or buggy code,
either (un)intentionally included in homework assignments, or, generated by
themselves, if they cannot identify where code will fail without executing
it.

# The Conclusion

Students must develop a coherent and strong internal model of code execution
to allow them to understand code flow, fix broken code, and debug issues.

# My Hypothesis

Augmenting lessons with:

- Tracing - Stepping through the internal state
- Faded examples
- Debugging intentionally broken examples

Will give students enough tools to respond dynamically to failure
states with informed experience to resolve issues they encounter as programmers.

# Opportunities for Improvement

## Mental Model

The student's mental model of the code underlies everything they do as a programmer, from conception to implementation to debugging to their self efficacy:


> This study shows that a well-developed and accurate mental model directly
> affects course performance and also increases self efficacy, the other key
> element in course performance. Given this double impact, helping students
> develop good mental models should remain a goal in introductory programming
> courses. [@Ramalingam_2004]

This is a foundational skill to be able to *think* through a program, step by step, and understand how the code executes and which variables exist when, and what their values should be. This mental modelling allows students to predict the behaviour of a system, and when it diverges from their prediction, recognise any potential bugs.


## Tracing

\begin{figure}[ht]
\begin{minipage}[b]{0.65\linewidth}
\begin{lstlisting}
# Initialise our accumulator
x = 1 + 1
# Loop over our input data
for i in range(10): # 0..9
    # In-loop temporary variable
    tmp = x * 2 + i
    # Update our accumulator
    x = tmp + 1
# Output our result
print(f'The final value is {x}')
\end{lstlisting}
\caption{Example Python Code. While not representative of a real world workflow, it is useful as an illustrative example, and an example which can be given directly to beginners for them to attempt.\label{fig:code}}
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.25\linewidth}
    \begin{tabular}{r|lll}
        Line & \texttt{i} & \texttt{x} & \texttt{tmp} \\\hline
        2    & -          & 2          & - \\
        4    & 0          & 2          & - \\
        6    & 0          & 2          & 4 \\
        8    & 0          & 5          & 4 \\
        4    & 1          & 5          & - \\
        6    & 1          & 5          & 11 \\
        8    & 1          & 12         & 11 \\
        4    & 2          & 12         & - \\
        6    & 2          & 12         & 26 \\
        8    & 2          & 27         & 26 \\
\end{tabular}
\caption{Example of a student's process tracing execution flow}
\label{fig:table}
\end{minipage}
\end{figure}

While there is no bug in \cref{fig:code}, when there *is* a bug present, having students produce a table like \cref{fig:table} significantly improves their understanding of code flow and execution [@Hertz_2013]. "Tracing" is a valuable and easy to complete exercise, and the results can even be checked automatically leading to good scalability of the exercise across larger classes.

## Faded Examples

Given that the students taught by Avans are primarily novice programmers who have not read or written a programming language before, we need to take significant care of their cognitive load. Both learning based on problem-solving and worked examples may cause high cognitive loads for different audiences, and exploring alternatives is important [@Retnowati_2017]. Faded examples such as what is seen in \cref{fig:fade} are exactly such an alternative, starting with a fully worked example and removing successive components until we reach a problem description requiring a full solution. This leads to fewer unproductive learning events [@Renkl_2004].

\begin{figure}[ht]
    \begin{minipage}[b]{\linewidth}
\begin{lstlisting}
# Write a function that multiplies two numbers
def multiply(a, b):
    c = a * b
    return c
\end{lstlisting}
    %\caption{The initial problem shows the entire solution to students}
    \end{minipage}

    \begin{minipage}[b]{\linewidth}
\begin{lstlisting}
# Write a function that adds two numbers
def add(___):
    ____
    return c
\end{lstlisting}
        %\caption{Increased fading, here we call out blanks students should fill in specifically with syntactically incorrect underscores.}
    \end{minipage}

    \begin{minipage}[b]{\linewidth}
\begin{lstlisting}
# Write a function that subtracts two numbers
\end{lstlisting}
    %\caption{Final fading, the entire problem is gone except for the description of what they need to do.}
    \end{minipage}

  \caption{Example of fading in coding exercises, these let students work on a continuum from worked out examples to self-devised solutions.\label{fig:fade}}
\end{figure}

Faded examples however, do come at a higher cost of implementation than worked out examples [@Zamary_2018]. They require writing the correct worked out example and then determining which components to remove, which presents an additional cost during course updates that if examples are changed they need to be double checked to ensure they are still valid, whereas worked examples can be checked more automatically.


## Debugging

Debugging is the act of identify and resolving "bugs" or defects within code, a term popularly attributed to my personal hero Admiral Grace Hopper:

> While she was working on a Mark II computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were "debugging" the system [@enwiki:1069955193]

Debugging also functions as a reinforcement method we can use once students have an ok mental model of code execution, a necessary pre-requisite for this activity, which can be further developed through debugging [@Ramalingam_2004] alongside their self-efficacy [@Michaeli_2019]. Debugging activities can take many forms but most commonly the task is to correct incorrect code, an activity that works best if they are primed with a number of methods of debugging [@Murphy_2008] such as the "Wolf Fence" [@Gauss_1982], commenting out code, or breakpoints.

\begin{figure}[]
\begin{lstlisting}
# Fix me!
for number in range(10):
    # use a if the number is a multiple of 3, otherwise use b
    if Number \% 3 == 0:
        message = message + a
    else:
        message = message + "b"
print(message)
\end{lstlisting}
\caption{A debugging exercise featuring code with numerous issues from type confusion, variable typos, and failure to initialise a variable.\label{fig:debug}}
\end{figure}


## Pair Programming

![Pair programming has two participants join forces to develop solutions, one leading the effort, one writing the code. (Image: StartupStockPhotos.com / CC0)\label{fig:pair}](./pair-programming.jpg)

Complementary to the efforts of Mental Model development via Tracing/Debugging/Fading, pair programming or "pairing" (\cref{fig:pair}) provides a reinforcement activity where they utilise similar skills. As one person writes and executes code, the other person `drives' the experience, telling them what to write [@Williams;Williams_2001]. It has become a common learning model in introductory courses due to its benefits to students [@mendes2005investigating;mendes2006replicated;Hannay_2007]. Specifically this technique has also been shown to be beneficial for women in computer science and gives them better chances for success in future programming endeavours [@werner2004pair]. Adopting this technique already (Assignment 6) has shown initially promising results, provided we adhere to principles outlined by  [@Mentz_2008].

# The Intervention

The intervention will consist of overhauling the formative assessments that students complete during class to include these new types of problems in the daily curriculum. The example code listings provided (\cref{fig:code,fig:fade,fig:debug}) are representative of the updates to formative assessments that we will make.

- This intervention is well backed by research showing improvements in student proficiency and grades [@Hertz_2013;Renkl_2002] leading to better learning outcomes [@Renkl_2004].
- These skills significantly improve self efficacy [@Ramalingam_2004;Michaeli_2019].

The second point, lack of self-efficacy has been a significant cost for TOAs and teachers alike in terms of interruptions for questions students should be able to answer on their own, and by applying these interventions hopefully students will find themselves and peers a more reliable source of discussion and answers. Self empowerment in this field sets the students on a good path for their future.

# Assessment

It is difficult to conduct a sufficiently controlled experiment given individual teacher preferences and teaching styles, so assessment will be carried out via review of student homework solutions. The system we use that collects student homework submission tracks student code entry over time, and will let us review how they attempted solutions and let us make qualitative observations.


# The Awaited Results

![Diagram of the PRIMM methodology [@Sentance_2017]](./primm.png)

Unfortunately there was not sufficient time to trial all of these strategies and evaluate the sections. Instead, all of these are currently being trialled in Group 32MBI02, please check back at the end of P3 for results. It is expected that these research backed teaching strategies will significantly improve student learning.

However, given that existing similar models, incorporating all of these
activities, is used in K-12 teaching [@Sentance_2017], this
intervention is expected to produce good results. Their model, PRIMM, starts with a good mental model which is required to predict, tracing during investigation, and debugging to modify code, all building towards students making things themselves.
